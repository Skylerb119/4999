import pandas as pd
import numpy as np
from datetime import datetime

# Define a function to reduce the memory usage of DataFrame columns
def reduce_memory_usage(df):
    start_mem = df.memory_usage().sum() / 1024**2
    print(f'Memory usage of the dataframe is {start_mem:.2f} MB')
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print(f'Memory usage after optimization is: {end_mem:.2f} MB')
    print(f'Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%')
    
    return df

# Define a function to read and process each file
def read_and_process(file_name):
    # Read the file, set low_memory to False to prevent mixed type inference
    df = pd.read_csv(file_name, low_memory=False)
    df = reduce_memory_usage(df)
    return df

"""Joining on case_id; read in each file, reduce memory usage, and join it to the base dataframe"""
file_names = [
    'train_applprev_1_0.csv', 'train_applprev_1_1.csv', 'train_applprev_2.csv',
    'train_base.csv', 'train_credit_bureau_a_1_0.csv', 'train_credit_bureau_a_1_1.csv',
    'train_credit_bureau_a_1_2.csv', 'train_credit_bureau_a_1_3.csv', 'train_credit_bureau_a_2_0.csv',
    'train_credit_bureau_a_2_1.csv', 'train_credit_bureau_a_2_2.csv', 'train_credit_bureau_a_2_3.csv',
    'train_credit_bureau_b_1.csv', 'train_credit_bureau_b_2.csv', 'train_debitcard_1.csv',
    'train_deposit_1.csv', 'train_other_1.csv', 'train_person_1.csv', 'train_person_2.csv',
    'train_static_0_0.csv', 'train_static_0_1.csv', 'train_static_cb_0.csv',
    'train_tax_registry_a_1.csv', 'train_tax_registry_b_1.csv', 'train_tax_registry_c_1.csv'
]
# Read and reduce memory usage of the base DataFrame
base_df = read_and_process(file_names[0])

# Iteratively merge files into the base DataFrame
for file_name in file_names[1:]:
    temp_df = read_and_process(file_name)
    # Merge while ensuring no overlapping columns other than 'case_id'
    base_df = base_df.merge(temp_df, on='case_id', how='left', suffixes=('', '_from_' + file_name.rstrip('.csv')))

# Function to preprocess the data
def preprocess_data(df):
    # Handle missing values for numeric columns by replacing them with the mean
    for column in df.select_dtypes(include=[np.number]).columns:
        if df[column].isnull().any():
            df[column].fillna(df[column].mean(), inplace=True)

    # Calculate age using 'DOB' columns if present
    if 'DOB' in df.columns:
        df['DOB'] = pd.to_datetime(df['DOB'])
        df['Age'] = df['DOB'].apply(lambda x: datetime.now().year - x.year)
    return df

# Apply the preprocessing function to the base DataFrame
base_df = preprocess_data(base_df)

# Drop rows with any missing values and remove duplicate rows
base_df.dropna(inplace=True)
base_df.drop_duplicates(inplace=True)

# Save the cleaned and merged dataset
base_df.to_csv('merged_test_cleaned.csv', index=False)

print("Data processing complete. The cleaned data is saved as 'merged_test_cleaned.csv'.")
